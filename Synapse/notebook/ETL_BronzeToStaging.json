{
	"name": "ETL_BronzeToStaging",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "decompressSmall",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b812f626-93b8-4e2a-9307-08c629171517"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/3b608a14-90b7-42d9-82e2-77c947974137/resourceGroups/DS562-Team-13/providers/Microsoft.Synapse/workspaces/team13-github-realtime-events/bigDataPools/decompressSmall",
				"name": "decompressSmall",
				"type": "Spark",
				"endpoint": "https://team13-github-realtime-events.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/decompressSmall",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 5
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"from pyspark.sql.functions import to_date\n",
					"import time\n",
					"import sys\n",
					"import logging\n",
					"from datetime import datetime, timedelta\n",
					"\n",
					"# Configure logging to flush immediately in Jupyter\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO,\n",
					"    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
					"    stream=sys.stdout,\n",
					"    force=True  # Ensures Jupyter respects the logging settings\n",
					")\n",
					"\n",
					"# Set up the configuration for accessing the storage account\n",
					"storage_account_name = \"team13adls\"\n",
					"container_name = \"github-realtime-issue\""
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"source": [
					"def read_github_data(year: int, month: int, day: int):\n",
					"    \"\"\"\n",
					"    Reads JSON.gz files for a specific year, month, and day.\n",
					"    Return: DataFrame containing the JSON data\n",
					"    \"\"\"\n",
					"    # Format month and day as two-digit strings\n",
					"    month_str = f\"{month:02d}\"\n",
					"    day_str = f\"{day:02d}\"\n",
					"    \n",
					"    # Construct the file path using f-string formatting\n",
					"    file_path = (\n",
					"        f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"        f\"bronze/GH_archive_raw/year={year}/month={month_str}/day={day_str}/*.json.gz\"\n",
					"    )\n",
					"    \n",
					"    # Read JSON files from the constructed path\n",
					"    df = spark.read.json(file_path)\n",
					"    return df"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"source": [
					"def transform_issues_event(df):\n",
					"    \"\"\"\n",
					"    Transforms the raw DataFrame by filtering for IssuesEvent,\n",
					"    selecting relevant columns, and converting timestamps.\n",
					"\n",
					"    :param df: Input DataFrame containing raw data.\n",
					"    :return: Transformed DataFrame with the selected columns and additional date column.\n",
					"    \"\"\"\n",
					"    transformed_df = df.filter(df[\"type\"] == \"IssuesEvent\").select(\n",
					"        col(\"actor\"),\n",
					"        col(\"created_at\"),\n",
					"        col(\"id\").alias(\"event_id\"),\n",
					"        col(\"org\"),\n",
					"        col(\"repo\"),\n",
					"        col(\"payload.action\"),\n",
					"        col(\"payload.issue\")\n",
					"    )\n",
					"\n",
					"    # Convert created_at to timestamp and create a date column\n",
					"    transformed_df = transformed_df.withColumn(\"created_at\", col(\"created_at\").cast(\"timestamp\"))\n",
					"    transformed_df = transformed_df.withColumn(\"date\", to_date(col(\"created_at\")))\n",
					"    \n",
					"    return transformed_df"
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"source": [
					"def load_to_delta_bronze_staging(df):\n",
					"    \"\"\"\n",
					"    Loads the transformed DataFrame into a Delta table partitioned by date.\n",
					"\n",
					"    :param df: Transformed DataFrame.\n",
					"    :param output_path: Destination path for the Delta table.\n",
					"    \"\"\"\n",
					"    bronze_staging_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze/staging/\"\n",
					"\n",
					"    df.write.format(\"delta\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"        .option(\"partitionOverwriteMode\", \"dynamic\") \\ \n",
					"        .option(\"mergeSchema\", \"true\") \\\n",
					"        .partitionBy(\"date\") \\\n",
					"        .save(bronze_staging_path)"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"source": [
					"def process_day(year:int, month:int, day:int, max_retries=2, sleep_interval=5):\n",
					"    \"\"\"\n",
					"    Processes GitHub data for a specific day: reads, transforms, and loads to Delta.\n",
					"    Retries the processing in case of failure.\n",
					"\n",
					"    Return: dict with status info (success or failure and error message if applicable)\n",
					"    \"\"\"\n",
					"    status = {\"year\": year, \"month\": month, \"day\": day, \"status\": None, \"error\": None}\n",
					"    attempt = 0\n",
					"    \n",
					"    while attempt < max_retries:\n",
					"        try:\n",
					"            logging.info(f\"Attempt {attempt + 1} for processing {year}-{month:02d}-{day:02d}\")\n",
					"            \n",
					"            # Read JSON files for the specific day\n",
					"            raw_df = read_github_data(year, month, day)\n",
					"            logging.info(\"Data read successfully.\")\n",
					"            \n",
					"            # Transform the DataFrame (filter for IssuesEvent, select columns, etc.)\n",
					"            transformed_df = transform_issues_event(raw_df)\n",
					"            logging.info(\"Transformation completed successfully.\")\n",
					"            \n",
					"            # Load to Delta Bronze Staging\n",
					"            load_to_delta_bronze_staging(transformed_df)\n",
					"            logging.info(\"Data loaded into Delta successfully.\")\n",
					"            \n",
					"            status[\"status\"] = \"success\"\n",
					"            return status  # Exit once successful\n",
					"            \n",
					"        except Exception as e:\n",
					"            attempt += 1\n",
					"            logging.error(f\"Error on attempt {attempt} for {year}-{month:02d}-{day:02d}: {e}\")\n",
					"            if attempt < max_retries:\n",
					"                logging.info(f\"Retrying after {sleep_interval} seconds...\")\n",
					"                time.sleep(sleep_interval)\n",
					"            else:\n",
					"                status[\"status\"] = \"failed\"\n",
					"                status[\"error\"] = str(e)\n",
					"    return status"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"source": [
					"def process_date_range(start_date: str, end_date: str, max_retries=2, sleep_interval=5):\n",
					"    \"\"\"\n",
					"    Processes GitHub data for a given date range: reads, transforms, and loads data to Delta.\n",
					"    The range includes both start_date and end_date. It logs and prints the status for each day as it runs.\n",
					"    \n",
					"    :param start_date: string in 'YYYY-MM-DD' format for the starting date.\n",
					"    :param end_date: string in 'YYYY-MM-DD' format for the ending date. (inclusive)\n",
					"    \"\"\"\n",
					"    # Convert string dates to datetime.date\n",
					"    start_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
					"    end_date = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
					"\n",
					"    current_date = start_date\n",
					"    while current_date <= end_date:\n",
					"        status = process_day(current_date.year, current_date.month, current_date.day, max_retries, sleep_interval)\n",
					"        message = f\"Processed {current_date}: {status['status']}\"\n",
					"        if status[\"status\"] == \"success\":\n",
					"            logging.info(message)\n",
					"        else:\n",
					"            logging.error(message)\n",
					"        print(message, flush=True)\n",
					"        sys.stdout.flush()\n",
					"        current_date += timedelta(days=1)\n",
					""
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"process_date_range(\"2024-03-28\",\"2024-04-30\")"
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"DESCRIBE HISTORY delta.`abfss://github-realtime-issue@team13adls.dfs.core.windows.net/bronze/staging/`;"
				],
				"execution_count": 104
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\n",
					"recovered_path = \"abfss://github-realtime-issue@team13adls.dfs.core.windows.net/bronze/recovered_staging/\"\n",
					"\n",
					"# Ensure the recovered table exists with partitioning by \"date\"\n",
					"try:\n",
					"    recovered_dt = DeltaTable.forPath(spark, recovered_path)\n",
					"except Exception:\n",
					"    empty_df = spark.read.format(\"delta\").load(bronze_staging_path).limit(0)\n",
					"    empty_df.write.format(\"delta\") \\\n",
					"        .mode(\"overwrite\") \\\n",
					"        .partitionBy(\"date\") \\\n",
					"        .save(recovered_path)\n",
					"    recovered_dt = DeltaTable.forPath(spark, recovered_path)"
				],
				"execution_count": 106
			},
			{
				"cell_type": "code",
				"source": [
					"# Get available versions from Delta history.\n",
					"history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{bronze_staging_path}`\")\n",
					"versions = history_df.select(\"version\").distinct().rdd.flatMap(lambda x: x).collect()\n",
					"\n",
					"# Process each version: each version is assumed to contain data for one date (or the partitions you want to update).\n",
					"for v in versions:\n",
					"    try:\n",
					"        print(f\"Processing version: {v}\")\n",
					"        df_version = spark.read.format(\"delta\").option(\"versionAsOf\", v).load(bronze_staging_path)\n",
					"        \n",
					"        # Write using dynamic partition overwrite.\n",
					"        df_version.write.format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
					"            .save(recovered_path)\n",
					"        \n",
					"        print(f\"Version {v} processed successfully.\")\n",
					"    except Exception as e:\n",
					"        print(f\"Skipping version {v} due to error: {e}\")\n",
					"\n",
					"print(\"Recovery complete!\")"
				],
				"execution_count": 109
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"DESCRIBE DETAIL delta.`abfss://github-realtime-issue@team13adls.dfs.core.windows.net/bronze/recovered_staging/`;"
				],
				"execution_count": 110
			},
			{
				"cell_type": "code",
				"source": [
					"spark.stop()"
				],
				"execution_count": 111
			},
			{
				"cell_type": "code",
				"source": [
					""
				]
			}
		]
	}
}