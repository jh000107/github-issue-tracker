{
	"name": "Model Training",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "909aba89-6ca5-4ddf-8792-591f4d87775d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"storage_account_name = \"team13adls\"\n",
					"container_name = \"github-realtime-issue\"\n",
					"dataset_path = \"gold/viz_issues_event/event_month=2024-01\"\n",
					"\n",
					"adls_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{dataset_path}\"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.parquet(adls_path)\n",
					"\n",
					"df.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, concat_ws\n",
					"\n",
					"df = df.withColumn(\"text\", concat_ws(\" \", col(\"issue_title\"), col(\"issue_body\")))\n",
					"df = df.dropna(subset=[\"text\", \"issue_resolve_time_label\"])\n",
					"df_clean = df.select(\"text\", \"issue_resolve_time_label\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
					"\n",
					"# Tokenize text\n",
					"tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
					"words_data = tokenizer.transform(df_clean)\n",
					"\n",
					"# Remove stop words\n",
					"remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
					"filtered_data = remover.transform(words_data)\n",
					"\n",
					"# TF → term frequency\n",
					"hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
					"featurized_data = hashing_tf.transform(filtered_data)\n",
					"\n",
					"# IDF → inverse document frequency\n",
					"idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
					"idf_model = idf.fit(featurized_data)\n",
					"rescaled_data = idf_model.transform(featurized_data)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"label_indexer = StringIndexer(inputCol=\"resolve_time_label\", outputCol=\"label\")\n",
					"rescaled_data = label_indexer.fit(rescaled_data).transform(rescaled_data)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.classification import LogisticRegression\n",
					"\n",
					"(training_data, test_data) = rescaled_data.randomSplit([0.8, 0.2], seed=42)\n",
					"\n",
					"lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
					"lr_model = lr.fit(training_data)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
					"\n",
					"predictions = lr_model.transform(test_data)\n",
					"evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
					"\n",
					"accuracy = evaluator.evaluate(predictions)\n",
					"print(f\"Test Accuracy = {accuracy:.4f}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"lr_model.write().overwrite().save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/models/lr_issue_classifier\")"
				],
				"execution_count": null
			}
		]
	}
}